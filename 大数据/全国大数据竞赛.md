# 竞赛内容

## 大数据系统搭建

### 基础环境配置

### 数据库环境配置

### 分布式网络环境配置

### Zookeeper集群环境搭建

### Hadoop分布式集群搭建

Hadoop 集群具体来说包含两个集群：**HDFS集群**和**YARN集群**，两者逻辑上分离，但物理上常在一起。 另外，对于Hadoop的集群来讲，可以分为两大类角色：**master**和**slave**

- **HDFS 集群**：**负责海量数据的存储**，集群中的角色主要有：**NameNode**（一个,master）、**DataNode**（若干,slave）和**SecondaryNameNode**（一个）
- **YARN 集群**：**负责海量数据运算时的资源调度**，集群中的角色主要有： **ResourceManager** （一个,master） 和 **NodeManager**（若干,slave）

Hadoop集群的部署方式分为3种，分别是单机模式、伪分布式模式和完全分布式模式

- 单机模式：即独立模式，在该模式下，无需运行任何守护进程、所有的程序都在单个JVM上执行
- 伪分布式模式：Hadoop程序的守护进程运行在一台主机节点上
- 完全分布式模式：Hadoop的守护进程分别运行在由多个主机搭建的集群上，不同节点担任不同的角色

#### JDK安装

环境环境变量配置

打开配置文件

vim /etc/profile

加入内容

export JAVA_HOME=/root/software/jdk1.8.0_221 #配置Java安装目录

export PATH=$PATH:$JAVA_HOME/bin #在原PATH的基础上加入JDK的bin目录

配置文件生效

source /etc/profile

#### ssh免密

生成密钥对

`ssh-keygen`

之后id_rsa和id_rsa.pub将会被创建到/root/.ssh目录中

将公钥放置到授权列表文件authorized_keys中

`cp id_rsa.pub authorized_keys`

权限设置仅拥有者可读可写，其他人无权限

`chmod 600 authorized_keys`

验证

`ssh localhost`

#### HDFS伪分布式集群

配置环境变量hadoop-env.sh

JDK安装位置 echo $JAVA_HOME

打开hadoop-env.sh修改JAVA_HOME参数为本机JDK安装位置

#### 配置核心组件core-site.xml

目的是配置HDFS地址、端口号，以及临时文件目录

```xml
<configuration>
<!--HDFS集群中NameNode的URI-->
	<property>
	<!--NameNode地址-->
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9000</value>
	</property>
	<!--默认的HDFS路径-->
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/hadoop/data/tmp</value>
	</property>
	<!--hadoop的临时目录，如果需要配置多个目录，需要逗号隔开-->
	<property>
	<name>ha.zookeeper.quorum</name>
	<value>hadoop01:2181,hadoop02:2181,hadoop03:2181</value>
	</property>
	<!--配置Zookeeper 管理HDFS-->
</configuration>
```

#### 配置文件系统hdfs-site.xml

```xml
<!--NameNode在本地文件系统中持久储存命名空间和事务日志的路径-->
<property>
		<name>dfs.namenode.name.dir</name>
		<value>/root/hadoopData/name</value>
</property>
<!--DataNode在本地文件系统中存放块的路径-->
<property>
		<name>dfs.datanode.data.dir</name>
		<value>/root/hadoopData/data</value>
</property>
<!--数据块副本的数量，默认为3-->
<property>
		<name>dfs.replication</name>
		<value>1</value>
</property>
```

#### 系统环境变量配置

#配置Hadoop安装目录

`export HADOOP_HOME=/root/software/hadoop-2.7.7`

#在原PATH的基础上加入Hadoop的bin目录和sbin目录

`export PATH=$PATH:$HADOOP_HOME/bin::$HADOOP_HOME/sbin`

#### slaves文件配置
用于记录Hadoop集群的从节点的主机名

#### 配置Hadoop系统环境变量
#### 配置Hadoop的安装目录、

`export HADOOP_HOME=/root/software/hadoop-2.7.7`

#### 在原PATH的基础上加入Hadoop的bin和sbin目录

`export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin`

#### HDFS集群测试

- 格式化
    - `hdfs namenode -format`
- 启动namenode
    - `hadoop-daemon.sh start namenode`
- 启动datanode
    - `hadoop-daemon.sh start datanode`
- 启动secondarynamenode
    - `hadoop-daemon.sh start secondarynamenode`

### YARN伪分布式集群

#### 配置环境变量yarn-env.sh

将yarn-env.sh文件中JAVA_HOME参数位置改为本机的JDK安装的位置

#### 配置计算框架

- 进入`$HADOOP_HOME/etc/hadoop/`目录下将mapred-site.xml.template文件复制并改名为mapred-site.xml。
- 打开mapred-site.xml文件，进入编辑模式。

```xml
<!-- 指定使用YARN运行MapReduce程序，默认为 local --> 
<property> 
<name>mapreduce.framework.name</name> 
<value>???</value> </property>
```

#### 配置YARN系统

- 打开YARN核心配置文件yarn-site.xml
- 在文件`<configuration></configuration>`中间添加配置内容：

```xml
<!-- NodeManager上运行的附属服务，也可以理解为 reduce 获取数据的方式 --> 
<property> 
<name>yarn.nodemanager.aux-services</name> 
<value>mapreduce_shuffle</value> 
</property>
```

#### 启动检测YARN集群

- 使用脚本命令一键启动YARN集群。
- 查看进程是否启动ResourceManager和NodeManager
- 使用脚本命令一键关闭YARN集群

```xml
# 常用参数
1.mapreduce.job.hdfs-servers
value：${fs.defaultFS}   
说明：job客户端参数。

2.mapreduce.framework.name
value：yarn
说明：指定分布式计算使用的框架是yarn，另外的可用值还有 local（本地的作业运行器）和classic（MR1运行模式），默认为 local。

3.mapreduce.task.io.sort.mb
value：100
说明：排序文件时使用的缓冲区内存总量，以兆字节为单位。 默认情况下，为每个合并流提供1MB，这应该最小化搜索。

4.mapreduce.map.sort.spill.percent
value：0.8
说明：Map阶段溢写文件的阈值（排序缓冲区大小的百分比）。

5.mapreduce.jobtracker.address
value：local
说明：MapReduce作业跟踪器运行的主机和端口。 如果是“本地”，则作业将作为单个映射在进程中运行并减少任务。

6.mapreduce.job.maps
value:2
说明：单个任务的map数量。

7.mapreduce.job.reduces
value：1
说明：单个任务的reduce数量。

 8.mapreduce.job.running.map.limit
value：0
说明：单个任务并发的最大map数，0或负数没有限制

9.mapreduce.job.running.reduce.limit
value：0
单个任务并发的最大reduce数，0或负数没有限制

10.mapreduce.job.max.map
value：-1
说明：单个任务允许的最大map数，-1表示没有限制。

11.mapreduce.job.max.split.locations
value：10
说明：分片数量

12.mapreduce.job.split.metainfo.maxsize
value:10000000
说明：split的元数据信息数量，如果value为-1，则没有限制

13.mapreduce.map.maxattempts
value：4
说明：每个 Map Task 最大重试次数,一旦重试参数超过该值,则认为 Map Task 运行失败。

14.mapreduce.reduce.maxattempts
value：4
说明：每个 reduce Task 最大重试次数,一旦重试参数超过该值,则认为 reduce Task 运行失败。

15.mapreduce.reduce.shuffle.parallelcopies
value：5
说明：Reduce Task启动的并发拷贝数据的线程数目

 16.mapreduce.task.timeout
value：600000
说明：如果任务既不读取输入，也不写入输出，也不更新其状态字符串，则任务终止之前的毫秒数。0表示禁用超时。

17.mapreduce.map.memory.mb
value：1024
说明：每个Map Task需要的内存量

18.mapreduce.map.cpu.vcores
value：1
说明：每个Map Task需要的虚拟CPU个数

19.mapreduce.reduce.memory.mb
value：1024
说明：每个Reduce Task需要的内存量

20.mapreduce.reduce.cpu.vcores
value：1
说明：每个Reduce Task需要的虚拟CPU个数

21.mapred.child.java.opts
value：-Xmx200m
说明：jvm启动的子线程可以使用的最大内存。建议值-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc

22.yarn.app.mapreduce.am.resource.mb
value：1536
说明：MR ApplicationMaster占用的内存量。

23.yarn.app.mapreduce.am.resource.cpu-vcores
value：1
说明：MR ApplicationMaster占用的虚拟CPU个数。

24.mapreduce.jobhistory.address
value：0.0.0.0:10020
说明：指定历史服务器的地址和端口

25.mapreduce.jobhistory.webapp.address
value：0.0.0.0:19888
说明：历史服务器的web地址

26.mapreduce.map.java.opts
value：-Xmx1024m
说明：每个map任务的jvm需要的内存量，一般为map内存的0.8

27.mapreduce.reduce.java.opts
value：
说明：每个reduce任务jvm需要的内存量。为reduce内存量的0.8

28.mapreduce.map.output.compress
value：false
说明： map输出是否进行压缩，如果压缩就会多耗cpu，但是减少传输时间，如果不压缩，就需要较多的传输带宽,默认是false。配合 mapreduce.map.output.compress.codec使用

29.mapreduce.map.output.compress.codec
value：org.apache.hadoop.io.compress.DefaultCodec
```

### Hbase列式数据环境搭建

#### 单机安装

在 hbase-1.4.10/conf下对hbase-env.sh

`export JAVA_HOME=/root/software/jdk1.8.0_221`

使用自带ZK

`export HBASE_MANAGES_ZK=false`

#### hbase-site.xml

```xml
<property>
<name>hbase.rootdir</name>
<value>/root/software/hbase-1.4.10</value>
</property>
<property>
<name>hbase.zookeeper.property.dataDir</name>
<value>/root/software/hbase-1.4.10/zData</value>
</property>
<property>
<name>hbase.zookeeper.property.clientPort</name>
<value>2182</value>
</property>
```

bin/start-hbase.sh

jps

#### 伪分布式安装

hbase-env.sh
```sh
export JAVA_HOME=/root/software/jdk1.8.0_221
export HBASE_MANAGES_ZK=true
```

conf/hbase-site.xml

```xml
<property>
<name>hbase.rootdir</name>
<value>hdfs://localhost:9000/hbase</value>
</property>
<property>
<name>hbase.zookeeper.property.dataDir</name>
<value>/root/software/hbase-1.4.10/zData</value>
</property>
<property>
<name>hbase.zookeeper.property.clientPort</name>
<value>2181</value>
</property>
<property>
<name>hbase.cluster.distributed</name>
<value>true</value>
</property>
```
### Hive环境配置

### Spark集群部署

## 大数据平台管理与运维

### 自动部署

### 配置管理

### 监控管理

### 服务监控

### 动态扩展

## 大数据处理与应用(jupyter代码完形填空)

### 网络数据采集

### 数据持久化储存

### 数仓分层

### 数据预处理

### 数据计算

### 数据查询