# 大数据概述

典型的大数据平台包括Hadoop、Spark、Flink以及Flume/Kafka等集群

**大数据平台架构**

# hadoop概述

- **HDFS:** 分布式文件存储
- **YARN:** 分布式资源管理
- **MapReduce:** 分布式计算
- **Others:** 利用YARN的资源管理功能实现其他的数据处理方式

![](attachments/Pasted%20image%2020230926083340.png)
## 目录结构

**bin** ：Hadoop最基本的管理脚本和使用脚本的目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用Hadoop

**etc** : Hadoop配置文件所在的目录，包括core-site.xml、hdfs-site.xml\mapred-site.xml等从Hadoop1.0继承而来的配置文件和yarn-site.xml等Hadoop2.0新增的配置文件

**include** : 对外提供的编程库头文件(具体动态和静态库在lib目录中)，这些头文件均是由C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序

**lib** : 该目录包含了Hadoop对外提供的编程动态库和静态库，与include目录中的头文件结合使用

**libexec** : Hadoop管理脚本所在的目录，主要包含HDFS和YARN中各类服务的启动/关闭脚本

**share** : Hadoop各个模板编译后的jar包所在的目录，官方自带示例

## 核心配置文件

- hadoop-env.sh
- core-site.xml
- hdfs-site.xml
- mapred-site.xml
- yarn-site.xml
- workers 文件里记录的是集群主机名

## 基本组成-HDFS分布式文件系统

- master/slave架构
- 分布储存block
- 名字空间(NameSpace)
- 元数据管理Namenode
- 数据储存Datanode
- 副本

## HDFS概述

三大组件：

- NameNode
- DataNode
- Secondary NameNode

![](attachments/Pasted%20image%2020230926082606.png)
### NameNode

- NameNode也称为Master，仅储存HDFS的元数据：文件系统的目录树，并跟踪整个集群中的文件
- NameNode不储存实际数据或数据集。数据本身实际储存在DataNode中
- NameNode知道HDFS中任何给定文件的块列表及其位置。使用该信息NameNode知道如何从块中构建文件
- NameNode并不持久化储存每个文件中各个块所在的DataNode的位置信息，这些信息会在系统启动时从数据节点重建
- NameNode对于HDFS至关重要，当NameNode关闭时，HDFS/Hadoop集群无法访问
- NameNode是Hadoop中的单点故障
- NameNode所在机器通常会配置有大量内存(RAM)

![](attachments/Pasted%20image%2020230926082414.png)
### DataNode

- DataNode负责将实际数据储存在HDFS中
- DataNode也称为Slave NameNode和DataNode会保持不断通信
- DataNode启动时，它将自己发布到NameNode并汇报自己负责持有的块列表
- 当某个DataNode关闭时，它不会影响数据或集群的可用性。NameNode将安排由其他DataNode管理的块进行副本复制。DataNode所在机器通常配置有大量DataNode。因为实际数据储存在DataNode中
- DataNode会定期(dfs.heartbeat,interval配置项配置，默认为3秒)向NameNode发送心跳，如果NameNode长时间没有接受到DataNode长时间没有接收到DataNode发送的心跳，NameNode就会认为该DataNode失效
- block汇报时间间隔取参数dsf.blockport.intervaMsec，参数未配置的话默认为6小时

### SecondaryNameNode

**Checkpoint**详细步骤：

- NameNode管理着元数据信息，其中有两类持久化元数据文件：edits操作日志文件和fsimage元数据镜像文件。新的操作日志不会立即与fsimage进行合并，也不会刷到NameNode的内存中，而是会写到edits中（因为合并需要消耗大量的资源），操作成功之后更新至内存。
- 有dfs.namenode.checkpoint.period和dfs.namenode.checkpoint.txns两个配置，只要达到这两个条件任何一个，secondarynamenode就会执行checkpoint的操作
- 当触发checkpoint操作时，NameNode会生成一个新的edits即上图中的edits.new文件同时SecondaryNameNode会将edits文件和fsimage复制到本地
- SecondaryNameNode将下载下来的fsimage载入到内存，然后一条一条地执行edits文件，中的各项更新操作，使得内存中的fsimage保持最新，这个过程就是edits和fsimage文件合并，生成一个新的fsimage文件即上图中的Fsimage文件
- SecondaryNameNode节点的edits.new文件和Fsimage.ckpt文件会替代原来的edits文件和fsimage文件，至此刚好是一个轮回，即在NameNode中又是edits和fsimage文件
- 等待下一次checkpoint触发SecondaryNameNode进行工作，一直这样循环工作

![](attachments/Pasted%20image%2020230926082347.png)

### HDFS写数据流程

**详细步骤**：

- client发起文件上传请求，通过RPC与NameNode建立通讯，NameNode检查目标文件是否已存在，副父目录是否存在，返回是否可以上传；
- client请求第一个block该传输到哪些DataNode服务器上；
- NameNode根据配置文件中指定的备份数量及副本放置策略进行文件分配，返回可用的DataNode的地址，如：A,B,C；
- client请求3台DataNode中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将整个pipeline建立完成，后逐级返回client；
- client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位（默认64K），A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答。
- 数据被分割成一个个packet数据包在pipeline上依次传输，在pipeline反方向上，逐个发送ack（命令正确应答），最终由pipeline中第一个DataNode节点A将pipeline发送给client
- 当一个block传输完成之后，client再次请求NameNode上传第二个block到服务器

### HDFS读数据流程

**详细步骤**：

- Client向NameNode发起RPC请求，来确定请求文件block所在的位置
- NameNode会视情况返回或者全部block列表，对于每个block，NameNode都会返回含有该block副本的DataNode地址
- 这些返回的DN地址，会按照集群拓扑结构得出DataNode与客户端的距离，然后进行排序，排序两个规则；网络拓扑结构中距离Client近的排靠前；心跳机制中超时汇报的DN状态为STALE，这样的排靠后；
- Client选取考前的DataNode来读取block，如果客户端本身就是DataNode，那么将从本地直接获取数据；底层上本质是建立SocketStream（FDataInputStream），重复的调用父类DataInputStream的read方法，直到这个块上的数据读取完毕；
- 当读完列表的block后，若文件读取还没有结束，客户端会继续向NameNode获取下一批的block列表
- 读取完一个block都会进行checksum验证，如果读取DataNode时出现错误，客户端会通知NameNode，然后再从下一个拥有该block副本的DataNode继续读取、
- read方法是并行的读取block信息，不是一块一块的读取；NameNode只是返回Client请求包含的DataNode地址，并不是返回请求块的数据

### HDFS其他功能

**不同集群之间的数据复制**

- 集群内部文件拷贝scp

```bash
cd /export/sortwares/
scp -r jdk-8u141-linux-x64.tar.gzroot@node2:/export/
```

- 跨集群之间的数据拷贝distcp

```bash
bin/hadoop distcp hdfs://node1:8020/jdk-8u141-linux-x64.tar.gz 
hdfs://cluster2:9000/
```

**Archive档案的使用**

**HDFS安全模式**

NameNode主节点启动时，HDFS首先进入安全模式，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求

```bash
#手动进入安全模式
hdfs dfsadmin -safemode enter
#手动退出安全模式
hdfs dfsadmin -safemode leave
```

# MapReduce并行计算模型

## MapReduce概述

MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在Hadoop集群上

MapReduce所包含的思想分为两步： Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理，可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。

Reduce负责“合”，即对map阶段的结果进行全局汇总

## Maptask工作机制

- 读取数据组件InputFormat（默认TextInputformat）会通过getSplit方法对输入目录中文件进行逻辑切片规划得到block，有多少block就对应多少个MapTask
- 将输入文件切分为block之后，有RecordReader对象（默认为LineRecordReader）进行读取，以\n作为分隔符，读取一行数据，返回<key,value>,key表示每行首字母偏移量，value表示这一行文本内容
- 读取block返回<key,value>,进入用户自己继承Mapper类中，执行用户重写的map函数，RecordReader读取一行这里调用一次
- Mapper逻辑结束后，将Mapper的每条结果通过contextwrite进行collection数据收集。在collect中，会对其进行分区处理，默认使用HashPartitioner MapReduce提供Partitioner接口，它的作用就是根据key或value及Reducer的数量来决定当前的这对输出数据最终应该交给哪个Reducetask处理，默认对key hash后再以reducer的数量取模。默认的取模方式只是为了平均Reducer的处理能力，如果用户自己对Partitioner有需求，可以定制并设置到job上
- 接下来，会将数据写入内存，内存中这片区域叫做环形缓冲区，缓冲区的作用是批量收集Mapper结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然，key和value值都会被序列化成字节数组 环形缓冲区其实是一个数组，数组中存放着key和value的序列化数据与key，value的元数据信息，包括Partition，key的起始位置、value的起始位置、以及value的长度。环形结构是一个抽象概念 缓冲区是有大小限制，默认为100MB.当Mapper的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程称为spill，中文可翻译为溢写，这个溢写是有单独线程完成。不影响往缓冲区写Mapper结果的线程。溢写线程启动时不应该阻止Mapper结果输出，所以整个缓冲区有个溢写的比例spillpercent。这个比例默认为0.8，也就是当缓冲区的数据已经到达阈值buffersize_spillpercent=100M_0.8=80M，溢写线程启动，锁定80M的内存，执行溢写过程，Mapper的数据结果还可以往剩下的20MB内存中写，互不影响
- 当溢写线程启动后，需要对着80M空间的key/Value对的Value加起来，减少溢写到磁盘的数据量。Combiner会优化MapReduce的中间结果，所以它在整个模型中会多次使用 那么哪些场景才能使用Combiner呢？从这里分析，Combiner的输出是Reducer的输入，Combiner绝不能改变最终的计算结果。Conbiner只应该用于那种Reduce的输入Key/Value类型与输出的Key/Value类型完全一致，且不影响最终结果的场景。比如累加，最大值等；Combiner的使用一定得慎重，如果用得好，它对Job执行效率有帮助，反之会影响Reducer的最终结果
- 合并溢写文件，每次溢写会在磁盘上生成一个临时文件（写之前判断是否有Combiner），如果Mapper的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个临时文件存在；当整个数据处理结束之后开始对磁盘中的临时文件进行Merge合并，因为最终的文件只有一个，写入磁盘，并且为这个文件提供了一个索引文件，以记录每个reduce对应数据的偏移量

## Reduce工作机制

Reduce大致分为copy、sort、reduce三个阶段，重点在前两个阶段。copy阶段包含一个eveFetcher来获取已完成的map列表，由Fetcher线程去copy数据，再此过程中会启动两个merge线程，分别为inMemoryMerger和onDiskMerger，分别将内存中的中的数据merge到磁盘和将磁盘中的数据进行merge。待数据copy完成之后，copy阶段就完成了，开始进行sort阶段，sort阶段主要是执行finaMerge操作，纯粹的sort阶段，完成后就是reduce阶段，调用用户定义的reduce函数进行处理

**详细步骤：**

- Copy阶段，简单地拉取数据。Reduce进程启动一些数据copy线程（Fetcher），通过HTTP方式请求maptask获取属于自己地文件。
- Merge阶段，这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活。merge有三种形式：从内存到内存；内存到磁盘；磁盘到磁盘。默认情况下第一种形式不启用。当内存中的数据量到达一定阈值，就启动内存到到磁盘的merge。与map端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的文件。
- 合并排序，把分散的数据合并成一个大的数据后，还会再对合并后的数据排序
- 对排序后的键值对调用reduce方法，键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对，最后把这些输出的键值对写入HDFS文件中

## MapReduce的Shuffle过程

map阶段处理的数据如何传递给reduce阶段，是MapReduce框架中最关键的一个流程，这个流程就叫做shuffle

shuffle：洗牌、发牌——（核心机制：数据分区，排序，分组，规约，合并等过程）

shuffle是Mapreduce的核心，它分布再Mapreduce的map阶段和reduce阶段。一般把从Map产生输出开始到Reduce取得数据作为输入之前的过程称为shuffle

- Collect阶段：将MapTask的结果输出到默认大小为100M的环形缓冲区，保存的是key/value，Partition分区信息等。
- Spill阶段：当内存的数据量达到一定的阈值的时候，就会将数据写入本地磁盘，在将数据写入磁盘前需要对数据进行一次合并操作，以确保一个MapTask最终只产生一个中间数据文件
- Copy阶段：ReduceTask启动Fetcher线程到已经完成MapTask的节点上复制一份属于自己的数据，这些数据默认会保存在内存的缓冲区中，当内存的缓冲区达到一定的阈值的时候，就会将数据写到自己的磁盘上
- Merge阶段：在ReduceTask远程复制数据时，会在后台开启两个线程对内存到本地的数据文件进行合并操作
- Sort阶段：在对数据进行合并的同时，会进行排序操作，由于MapTask阶段已经对数据进行了局部的排序，ReduceTask只需保证Copy的数据的最终有效性即可。shuffle中的缓冲区大小会影响mapreduce程序的执行效率，原则上说，缓冲区越大磁盘io的次数越少执行，速度就越快

缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb

# Zookeeper分布式协调服务

**Zookeeper系统架构**

Zoopkeeper服务自身组成一个集群（2n+1个服务节点最多允许n个失效）。Zookeeper

服务有两种角色：一种是主节点（Leader），负责投票的发起和决议，更新系统状态；另一种是从节点（Fowwower），用于接收客户端请求并向客户端返回结果，在选主过程（即选择主节点的过程）中参与投票。主节点失效后，会从节中重新选举新的主节点。

客户端可以选择连接到Zookeeper集群中的每台服务器，而且每台服务端的数据完全相同，。每一个从节点都需要与主节点进行通信 ，并同步主节点上更新的数据

![](attachments/Pasted%20image%2020230926082300.png)
# yarn资源调度平台

## Yarn概述

Yarn是一个资源管理、任务调度的框架，主要包含三大模块：ResourceManger（RM）、NodeManager（NM），ApplicationMaster（AM）

ResourceManager负责所有资源的监控、分配和管理；

ApplicationManager负责每一个具体应用程序的调度和协调；

NodeMaster负责每一个节点的维护

对于所有的applications，RM拥有绝对的控制权和对资源的分配权。而每个AM则会和RM协商资源，同时和NodeManager通信来执行和监控task

可以把yarn理解为相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统上的应用程序，Yarn为这些程序提供运算所需的资源（内存、cpu）

- yarn并不清楚用户提交的程序的运行机制
- yarn只提供运算资源的调度（用户程序向yarn申请资源，yarn就负责分配资源）
- yarn中的主管角色叫ResourceManager
- yarn中具体提供运算资源的角色叫NodeManager
- yarn与运行的用户程序完全解耦，意味着yarn上可以运行各种类型的分布式运算程序，比如mapreduce，storm，spark，tez
- spark、storm等运算框架都可以整合在yarn上运行，只要他们各自的框架中有符合yarn规范的资源请求机制即可
- yarn成为一个通用的资源调度平台，企业中以前存在的各种运算集群都可以整合在一个物理集群上，提高资源利用率，方便数据共享

## Yarn三大组件

- ResourceManager
    - ResourceManager负责整个集群的资源管理和分配，是一个全局的资源管理系统
    - NodeManager以心跳的方式向ResourceManager汇报资源使用情况。RM只接受NM的资源回报信息。对于具体的资源处理则交给NM自己处理
    - YARN Scheduler根据application的请求为其分配资源，不负责application的请求为其分配资源，不负责application的监控、追踪、运行状态反馈、启动等工作
- NodeManager
    - NodeManager是每个节点上的资源和任务管理器，他是管理这台机器的代理，负责该节点资源的管理和监控。YARN集群每个节点都运行一个
    - NodeManager定时向ResourceManager汇报本节点资源（CPU,内存）的使用情况和Container的运行状态。当ResourceManager宕机时NodeManager自动连接RM备用节点
    - NodeManager接受并处理来自ApplicationManager的Contaioner启动、停止等各种请求
- ApplicationMaster
    - 用户提交的每个应用程序均包含一个ApplicationMaster，它可以运行在ResourceManager以外的机器上
    - 负责与RM调度器协商以获取资源（用Container表示）
    - 将得到的任务jiang'ying'bu（资源的二次分配）与NM通信以启动/停止任务
    - 监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务

## YARN运行机制

- client向RM提交应用程序，其中包括启动该应用的ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等
- ResourceManager启动一个container用于运行ApplicationMaster
- 启动中的ApplicationMaster向ResourceManager注册自己，启动成功后与RM保持心跳
- ApplicationMaster向ResrceManager发送请求，申请相应数目的container
- ResourceManager返回ApplicationMaster的申请的containers信息。申请成功的container，有ApplicationMaster进行初始化。container的启动信息初始化后，AM与对应的NodeManager通信，要求NM启动container。AM与NM保持心跳从而对NM上运行的任务进行监控和管理。
- container运行期间，ApplicationMaster对container进行监控。cotainer通过RPC协议向对应得AM汇报自己的进度和状态等信息
- 应用运行期间，client直接与AM通信获取应用的状态、进度更新等信息
- 应用运行结束后，ApplicationMaster向ResourceManager注销自己，并允许属于它的container被收回

# 大数据生态
![](attachments/Pasted%20image%2020230926082843.png)
# SSH免密登录

## 原理
![](attachments/Pasted%20image%2020230926082911.png)
# 集群搭建

## 集群部署规划
![](attachments/Pasted%20image%2020230926082931.png)
**注意：**

1. NameNode和SecondaryNameNode不要安装在同一台服务器
2. ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上

[Hadoop技术与大数据应用](https://www.notion.so/Hadoop-f152bcf534404820bfe7171ca5453bfc?pvs=21)

# Mapreduce程序案例分析

## WordCount

**业务逻辑**
![](attachments/Pasted%20image%2020230926083004.png)

```java
package com.ghgj.mazh.mapreduce.wc.demo1;
 
import java.io.IOException;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;
 
/**
 * 作者： 马中华：<http://blog.csdn.net/zhongqi2513>
 * 日期： 2017年6月29日 下午8:21:03 
 * 
 * 描述: MapReduce出入门：WordCount例子程序 
 */
public class WordCountMR {
 
	/**
	 * 该main方法是该mapreduce程序运行的入口，其中用一个Job类对象来管理程序运行时所需要的很多参数：
	 * 比如，指定用哪个组件作为数据读取器、数据结果输出器 指定用哪个类作为map阶段的业务逻辑类，哪个类作为reduce阶段的业务逻辑类
	 * 指定wordcount job程序的jar包所在路径 .... 以及其他各种需要的参数
	 */
	public static void main(String[] args) throws Exception {
		// 指定hdfs相关的参数
		Configuration conf = new Configuration();
		
		// 手动设置，该MapReduce程序读取的数据来自于HDFS集群
		conf.set("fs.defaultFS", "hdfs://hadoop02:9000");
		// 设置运行程序的用户是hadoop用户，就是安装hadoop集群的用户。如果该程序在Hadoop集群中使用hadoop用户进行运行，则可以去
		System.setProperty("HADOOP_USER_NAME", "hadoop");
		
		/**
		 * 以上的配置信息，事实上，在实际企业生产环境中，也可以使用conf.addResource方法进行加载。
		 * 当然如果配置文件的名字是core/hdfs/yarn/mapred-site/default.xml的话。 那么会自动加载的。
		 */
//		conf.addResource("hadoop_config/core-site.xml");
//		conf.addResource("hadoop_config/hdfs-site.xml");
 
		// 如果想让MR程序运行在特定的YARN集群之上，则可以使用一下代码，然后，这两信息，在安装集群的配置文件中都有配置的
		// conf.set("mapreduce.framework.name", "yarn");
		// conf.set("yarn.resourcemanager.hostname", "hadoop04");
 
		// 通过Configuration对象获取Job对象，该job对象会组织所有的该MapReduce程序所有的各种组件
		Job job = Job.getInstance(conf);
 
		// 设置jar包所在路径
		job.setJarByClass(WordCountMR.class);
 
		// 指定mapper类和reducer类
		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);
 
		/**
		 * 指定maptask的输出类型
		 * Mapper的输入key-value类型，由MapReduce框架决定， 默认情况下就是 LongWritable和Text类型
		 * 
		 * 假如 mapTask的输出key-value类型，跟reduceTask的输出key-value类型一致，那么，以上两句代码可以不用设置
		 */
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
 
		/**
		 * 指定reducetask的输出类型
		 * 如果reduceTask的输入key-value类型就是 mapTask的输出key-value类型。可以不需要指定
		 */
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
 
		// 为job指定输入数据的组件和输出数据的组件，以下两个参数是默认的，所以不指定也是OK的
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
 n
		// 为该mapreduce程序制定默认的数据分区组件。默认是 HashPartitioner.class
		job.setPartitionerClass(HashPartitioner.class);
 
		/**
		 * 指定该mapreduce程序数据的输入和输出路径:
		 * inputPath目录可以是文件，也可以是目录
		 * outputPath路径必须是不存在的目录
		 */
		Path inputPath = new Path("D:\\\\bigdata\\\\wordcount\\\\input\\\\wc.txt");
		Path outputPath = new Path("D:\\\\bigdata\\\\wordcount\\\\output");
 
		// 设置该MapReduce程序的ReduceTask的个数
		// job.setNumReduceTasks(3);
 
		// 该段代码是用来判断输出路径存在不存在，存在就删除，虽然方便操作，但请谨慎
		FileSystem fs = FileSystem.get(conf);
		if (fs.exists(outputPath)) {
			fs.delete(outputPath, true);
		}
 
		// 设置wordcount程序的输入路径
		FileInputFormat.setInputPaths(job, inputPath);
		// 设置wordcount程序的输出路径
		FileOutputFormat.setOutputPath(job, outputPath);
 
		// job.submit();
		// 最后提交任务(verbose布尔值 决定要不要将运行进度信息输出给用户)
		boolean waitForCompletion = job.waitForCompletion(true);
		// 主线程程序根据MapReduce程序的运行结果成功与否退出。
		System.exit(waitForCompletion ? 0 : 1);
	}
 
	/**
	 * Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
	 * 
	 * KEYIN 是指框架读取到的数据的key的类型，在默认的InputFormat下，读到的key是一行文本的起始偏移量，所以key的类型是Long
	 * VALUEIN 是指框架读取到的数据的value的类型,在默认的InputFormat下，读到的value是一行文本的内容，所以value的类型是String
	 * KEYOUT 是指用户自定义逻辑方法返回的数据中key的类型，由用户业务逻辑决定，在此wordcount程序中，我们输出的key是单词，所以是String
	 * VALUEOUT 是指用户自定义逻辑方法返回的数据中value的类型，由用户业务逻辑决定,在此wordcount程序中，我们输出的value是单词的数量，所以是Integer
	 * 
	 * 但是，String ，Long等jdk中自带的数据类型，在序列化时，效率比较低，hadoop为了提高序列化效率，自定义了一套序列化框架
	 * 所以，在hadoop的程序中，如果该数据需要进行序列化（写磁盘，或者网络传输），就一定要用实现了hadoop序列化框架的数据类型
	 * 
	 * Long ----> LongWritable 
	 * String ----> Text 
	 * Integer ----> IntWritable 
	 * Null ----> NullWritable
	 */
	static class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
 
		/**
		 * LongWritable key : 该key就是value该行文本的在文件当中的起始偏移量
		 * Text value ： 就是MapReduce框架默认的数据读取组件TextInputFormat读取文件当中的一行文本
		 */
		@Override
		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
 
			// 切分单词
			String[] words = value.toString().split(" ");
			for (String word : words) {
				// 每个单词计数一次，也就是把单词组织成<hello,1>这样的key-value对往外写出
				context.write(new Text(word), new IntWritable(1));
			}
		}
	}
 
	/**
	 * 首先，和前面一样，Reducer类也有输入和输出，输入就是Map阶段的处理结果，输出就是Reduce最后的输出
	 * reducetask在调我们写的reduce方法,reducetask应该收到了前一阶段（map阶段）中所有maptask输出的数据中的一部分
	 * （数据的key.hashcode%reducetask数==本reductask号），所以reducetaks的输入类型必须和maptask的输出类型一样
	 * 
	 * reducetask将这些收到kv数据拿来处理时，是这样调用我们的reduce方法的： 先将自己收到的所有的kv对按照k分组（根据k是否相同）
	 * 将某一组kv中的第一个kv中的k传给reduce方法的key变量，把这一组kv中所有的v用一个迭代器传给reduce方法的变量values
	 */
	static class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
 
		/**
		 * Text key : mapTask输出的key值
		 * Iterable<IntWritable> values ： key对应的value的集合（该key只是相同的一个key）
		 * 
		 * reduce方法接收key值相同的一组key-value进行汇总计算
		 */
		@Override
		protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
 
			// 结果汇总
			int sum = 0;
			for (IntWritable v : values) {
				sum += v.get();
			}
			// 汇总的结果往外输出
			context.write(key, new IntWritable(sum));
		}
	}
}
```

>💡 Hadoop使用了一套独有的配置文件管理系统，并提供自己的API，即使用org.apache.hadoop.conf.Configuration处理配置信息 Hadoop的配置文件采用XML格式，根元素是configuration,一般只包含子元素property。每一个property元素就是一个配置项，配置文件不支持分层或分级。每一个配置项一般包括配置属性的名称name、值value和一个关于配置项的描述description；元素find和Java中的关键字final类似，意味着这个配置项是“固定不变的”。final一般不出现，但是在合并资源的时候，可以防止配置项的值被覆盖


[Hadoop Configuration详解](https://www.cnblogs.com/ostin/articles/6920420.html)

**摘要—Configuraion**

- XML格式
- 属性都是string型，但是值的类型可能为多种
- loadResource()合并资源时为一个配置时，如果包含了同样的配置项，而且前一个没有被标记为final，那么将被后面的覆盖
- 属性拓展
- 7个主要的非静态成员变量
    - loadDefaults用于确认是否加载默认资源
    - properties、overlay、findParamaters和配置项相关
    - classLoader—类加载器变量，用于加载指定类或相关资源

![](attachments/Pasted%20image%2020230926083037.png)
- 资源加载

**部分组件说明**

System.setProperty("HADOOP_USER_NAME", "用户名")

对应的是 hdfs文件系统目录下的路径：/user/用户名的文件夹名设置后java客户端将会以对应身份访问hdfs

# 虚拟机集群搭建

> 别用ubuntu，用ubuntu是撒比，用centos

**修改虚拟机的网络配置**

![](attachments/Pasted%20image%2020230926083225.png)

![](attachments/Pasted%20image%2020230926083246.png)


**修改ubuntu网络配置**

- 修改interfases

```bash
# 备份原有配置
root@ubuntu:~# cd /etc/network/
root@ubuntu:~# mv interfaces interfaces.bak
root@ubuntu:~# cp interfaces.bak interfaces
root@ubuntu:~# vim interfaces
```

```bash
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).
 
source /etc/network/interfaces.d/*
 
# The loopback network interface
auto lo
iface lo inet loopback
 
# The primary network interface
auto ens33
iface ens33 inet static
address 192.168.5.130
netmask 255.255.255.0
gateway 192.168.5.2
dns-nameserver 114.114.114.114
```

- 修改DNS设置

vim /etc/resolv.conf

修改为 nameserver 114.114.114.114

最后重启

---