# 机器学习

[线性代数基础](机器学习——线性代数基础/线性代数基础.md)

# 机器学习的重要元素

## 数据格式

在监督学习和无监督学习问题中，数据集被定义为具有m个特征的实数向量有限集合:

$X=\{\bar{x_1},\bar{x_2},...,\bar{x_n}\},其中\bar{x_i}\in\R^m$

因为方法可能是概率的，因而需要考虑从统计多变量

考虑含有k个变量的任意子集，有：

$P(\bar{x_1},\bar{x_2},...,\bar{x_k})=\prod\limits_{i=1}^kp(\bar{x_i})\\$相应输出可以是连续变量或类别。是连续变量时称为**回归**，是类别时称为**分类**

输出连续变量：

$Y=\{\bar{y_1},\bar{y_2},...,\bar{y_n}\},其中{y_i}\in(0,1)或y_i\in\R^+$

输出为类别：

$y_i \in \{红,黑,白,绿\}或y_i \in \{0,1,2,3\}$

> 当标签可以采用有限数量的值（例如，它是二进制或双极）时问题是离散的（也称为分类，考虑到每个标签通常与明确定义的类或类别相关联 ），而 $y_i \in \R$时问题是连续的
> 

定义通用回归器为一个输入值与连续输出相关联的向量值函数 $r(\cdot)$

 定义通用分类器为预测输出为类别（离散值）的向量值函数 $c(\cdot)$r'g

 如果它们还依赖内部参数向量来确定一个通用预测器，那么称这种方法为**参数学习**

$\begin{cases}\tilde{y}=r(\bar{x};\bar{\theta})\\\tilde{y}=c(\bar{x};\bar{\theta})\end{cases}$

 向量 $\theta$是所有模型参数的汇总，这些参数通常是我们要学习的唯一元素。事实上大多数模型假设一个不能修改的标准结构（即便有些特定的动态神经网络允许添加或删除计算单元），并且适应性仅依赖于可能的参数范围

非参数学习不会对预测函数族进行初始化假设（比如，上面的 $r(\cdot)$和 $c(\cdot)$的通用参数化形式）

一个常见的非参数族称为**基于实例的学习**，不需要预先计算参数值，而仅基于由训练样本（实例集）确定的假设进行实时预测。一个广泛使用的方法是采用半径固定的邻域概念，即在分类问题中，新的样本被已分类的训练样本自动包围，通过考虑附近的样本类别来确定输出的类别，下面讨论一个此类中非常重要的算法族：**基于核的支持向量机**

通用的参数训练应该找到在给定的特点训练数据集上能够最小回归/分类误差的最佳参数向量，并生成能对未知样本进行泛化的预测器。

如果含有加性噪声，则输出可以表示为：

$\begin{cases}\tilde{y}=r(\bar{x};\bar{\theta})+n(\mu;\sigma^2)\\\tilde{y}=c(\bar{x};\bar{\theta})+n(\mu;\sigma^2)\end{cases}$ $其中\mu=E[n]=0,\sigma^2=E[n^2]<<1$

 在上式中，期望输出可以表示一个较为精确的预测加上零均值和低方差的高斯噪声。训练任务变成优化参数来提高信噪比。当然，如果不具有零平均值（与其他X值无关）的项，那么可能意味着必须考虑一个隐含的趋势（可能是已被过早抛弃的特征）。另一方面，高噪声方差意味着X被污染，其测量值是不可靠的。

在无监督学习中，通常只有一个具有m维向量的输入集X，并且使用以下表达式定义聚类函数 $cl(\cdot)$（具有n个目标类）：

$k_t = cl(\bar{x};\bar{\theta}),其中k_t \in \{0,1,…,n-1\}$

 聚类算法试图发现样本间的相似性并相应地对它们进行分组。因此 $cl(\cdot)$将始终输出0到n-1之间的标签（或者在1和n之间），表示与样本x表示最匹配的的醋。由于x是假设从训练阶段使用的相同的数据生成过程中提取的，因此我门数学上接受结果在所达到的准确度极限内可靠。另一方面（在每个机器学习问题中都是如此），如果从完全不同的分布中抽取x，则任何的预测和随机预测无法区分

总的来说，就是当我们在需要处理特定样本时，必须确保使用从相同分布中提取的元素来训练模型

**多类策略**

到目前我们假设回归和分类的都是对m长度的向量进行操作，但产生单个值或单个标签（输入向量始终只与一个输出元素有关）。当输出类的数目大于1时，有两种策略来解决分类问题

- 一对多
最常见的策略，并在scikit-learn中广泛应用于大多数算法。如果输出有n个类，并行训练n个分类器可以将实际所属的类与剩余的那些类分开。属于相对轻量级的方法（最多需要n-1个过程来找到正确的类，因为它具有 $O(n)$的复杂度），因此通常是默认选项，并且不需要进一步·的操作
- 一对一
一对多的方案是维每两个类训练一个模型。算法的复杂度不再是线性的（实际是 $O(n^2)$），根据多数类的结果来决定哪个是正确的类，一般来说，选用这种方法，计算量大，只有当选用全部数据集效果不好时才考虑采用

## 可学习性

模型的学习分为两部分：

- 结构
由特定算法进行选择，通常是不变的（除了在模型提供重新建模功能的情况下）
- 参数
是优化的目标

考虑n个无界参数，共同组成一个n维空间（给参数加入边界约束将生成该空间的一个子空间）。在这个空间中，每个点与待估计函数的固定部分（以及参数的特定集合）共同构成参数学习的假设 $H$:

$H = \{\bar{\theta_1},\bar{\theta_2},…,\bar{\theta_n}\}$

 参数学习的过程的目标是找到预测误差最小、泛化能力足以避免过拟合的最佳假设。

- 如果将来的预测数据具有和训练样本一样的分布，复杂的模型可以捕获较低阶的模型可能丢弃的小的变化，其可能是一个很好的选择。而在这个例子中，线性（或较低阶）模型可能导致欠拟合，因为它无法精确的表现出数据小的趋势
- 如果将来的预测数据与训练样本局部分布可能不同，那么为保持整体趋势，则最好选择具有较高的残余错误分类误差以及更好的泛化能力的模型。使用仅仅关注训练数据的复杂模型可能导致过拟合

### 欠拟合和过拟合

机器学习模型的目的是找到将输入数据和输出数据近似关联的未知函数（对于分类器则称为分类）。虽然训练集通常表示样本的全局分布，但该集合不包含所有可能的数据，否则该问题可以通过一对一关联得到解决。同样的，我们不知道可能的函数的解析表达式，因此在训练时有必要考虑拟合模型，在未知输入时保持模型能够实现泛化。在这方面，引入模型的表征能力概念是有用的，因为它能够在数据集上学习少量/大量可能分布。显然，模型的低表征能力通常与较简单的模型相关联，例如无法解决非线性问题，而高表征（既是基础模型的函数又是参数的函数）将导致更复杂的分离超平面。

线性分类器等效于线性等式：$y=mx+q$

 在这种情况下，有两个参数，m和q，曲线永远不会改变其斜率（由m定义）。相反，第二个分类器可以想象为一个三次方程： $y=ax^3+bx^2+cx$

 现在，我们有四个参数和两个输入值的幂。这些条件可以允许对可以改变其斜率两次的函数进行建模，并且可以适应更复杂的场景。显然，我们可以通过考虑通用·多项式函数来继续这种分析：

$y=\sum\limits_{n=1}^{p-1}a_nx^n$

 复杂性（以及因此的容量）与p度成比例。加入多项式和非线性函数，我们可以获得极其复杂的表示，这些表示可以足够灵活的捕获非一般数据集的细节。但是重要的是要记住增加容量通常是不可逆的操作。换句话说，更复杂的模型总是会更复杂，即使更简单的模型可取。学习过程可以拉伸或弯曲曲线，但是它永远无法消除斜率变化。

- 欠拟合：意味着可能因为数据量有限，模型无法得到训练集所呈现的特征。
- 过拟合：模型拟合能力过剩，但是由于过多考虑数据的变化而导致泛化能力不足。模型可以将所有已知样本几乎关联到相应的输出值，但是当出现未知的输入时，相应的预测误差可能非常的高

亲你和的模型1通常具有高偏差，偏差被定义为参数 $\theta$的估计值与真实值之间的差值：

$偏差 [\tilde{\theta}] = E[\tilde{\theta}]-\bar{\theta}$da

 当偏差为零时，模型被定义为无偏。另一方面，偏差的存在意味着算法不能学习 $\theta$的可接受的表示。

 过拟合通常与高方差相关，定义如下：

$方差 [\tilde{\theta}] = E[{(\tilde{\theta}-E[\tilde{\theta}])}^2]$

 高方差是高表征能力的结果。该模型现在能够改变并多次变化其斜率，但是它不再是原来的简单表示。考虑到预测误差，欠拟合跟容易检测，而过拟合可能更难发现，因为它最初可能被认为是完美拟合的结果。事实上，在分类任务中，高方差模型可以更容易的学习到训练阶段使用的数据集的结构。但是由于过度的复杂性，它可能变得很特殊。这通常意味着它将以较低的准确度预测从未见过的样本，因为这些特征不能被识别为属于哪个类别。模型捕获每个小的变化，现在可以更自由的调整其分离面，因此这些特征不能被识别为属于哪个类别。模型捕获每个小的变化，现在可以更自由地调整其分离面，因此相似性（这是泛化能力的基础）更难以被检测到